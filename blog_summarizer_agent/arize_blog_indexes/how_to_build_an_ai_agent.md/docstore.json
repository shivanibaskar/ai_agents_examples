{"docstore/metadata": {"a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d": {"doc_hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431"}, "df8ec48c-95ba-4faa-9e89-b3e72cde44fe": {"doc_hash": "a75f8f99b567e952e71ac1fc6196defee371a512071be6a361af64d64c4f7982", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}, "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc": {"doc_hash": "43444781a9e9569bcbe99e6ce6c870072544d5bfe66584790202ec783e55070a", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}, "3020f785-8d80-42a7-a99f-a04ebd245dd4": {"doc_hash": "1b9fc447e6d61bc8892177af8727f07f2a42c0011d330ef72ebaf36a080a3555", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}, "024e5af6-e3db-4146-9402-05442865f3b9": {"doc_hash": "bdc73b51aaad7a46674aaca90e9d35e2aa74a8bb4e82120df00af94a353fa2e6", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}, "8e58b1ac-4c33-4696-8b5d-e50f7b74846e": {"doc_hash": "2e023f2d22b4e4f0722d0a38c61f6215d5a60e84b3d50463553e93226bca6148", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}, "1be8a2b8-fc43-43a9-bc6a-6eed85923147": {"doc_hash": "8a6fb9d616972b423ef4cd670a9be8544f75bece561e13c643ca3a741ee42c73", "ref_doc_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d"}}, "docstore/data": {"df8ec48c-95ba-4faa-9e89-b3e72cde44fe": {"__data__": {"id_": "df8ec48c-95ba-4faa-9e89-b3e72cde44fe", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc", "node_type": "1", "metadata": {}, "hash": "ec7b7acc751540ba12a3daf7fbd25e9972c77ff5d2496203e40862afb5821f70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to Build An AI Agent\n\nURL Source: http://arize.com/blog/how-to-build-an-ai-agent/\n\nPublished Time: 2025-02-18T20:01:34+00:00\n\nMarkdown Content:\nAn agent is a software system that orchestrates multiple processing steps\u2014 including calls to large language models\u2014to achieve a desired outcome. Rather than following a linear, predefined path, an agent can traverse a wide solution space, handling decision-making logic, remembering intermediate steps, and determining which actions to take and in which order. This enables agents to complete difficult, broad-ranging tasks that are out of the realm of possibility for traditional software.\n\nIn this post we\u2019ll cover everything you need to know to build your own functioning AI agent using [smolagents](https://github.com/huggingface/smolagents), [AutoGen](https://github.com/microsoft/autogen), or [LangGraph](https://github.com/langchain-ai/langgraph).\n\nDo You Need an Agent?\n---------------------\n\nIt\u2019s not always necessary to implement an agent. It\u2019s always worth pausing to consider, do I really need an agent for this use case? Agents shine when your application requires iterative workflows, adaptive logic, or exploring multiple pathways.\n\nIf you\u2019re not sure, ask yourself:\n\n*   Iterative Flows: Does your workflow rely on multi-stage processes where each step informs the next?\n*   Adaptive Logic: Do you switch actions based on feedback or prior outcomes?\n*   Complex State Management: Do you have a range of possible actions that aren\u2019t limited to a single sequence?\n\nFor simpler use cases\u2014like basic queries\u2014an LLM alone may be enough. But agents offer:\n\n*   Memory & Planning: Track previous steps and map out what comes next.\n*   Tool Access: Integrate with APIs or the internet for up-to-date info.\n*   Longevity & Learning: Improve over time through iterative feedback.\n\nUltimately, the decision depends on your task\u2019s complexity, available resources, and the added value an agent can bring.\n\nLLM Function Calling: Single-Step vs. Full Agent\n------------------------------------------------\n\nA key ingredient in many agent systems is function (tool) calling, where an LLM outputs structured data that maps to specific actions or APIs. This can range from a simple one-step flow\u2014just a single LLM call deciding which function to invoke\u2014to a more advanced, multi-step agent with memory and planning.\n\n### What is Function Calling?\n\nModern LLMs like GPT-4, Claude, and Llama can produce responses in specific structured formats (e.g., JSON), which an application can parse to decide which \u2018tool\u2019 (function, API, service) to execute. This process bridges natural language with programmatic actions.\n\n*   Single-Step Choice: After giving an LLM a list of possible functions, the model selects the appropriate function based on user input.\n*   Structured Output: The LLM\u2019s response follows a specific format, making it easier to parse and reducing errors.\n*   Scalability: Over time, you can add more functions to handle additional tasks without altering core logic.\n\n### Single LLM Call vs. Full Agent\n\nFor more complex tasks, you can expand the LLM\u2019s role to include iterative reasoning and tool usage:\n\n*   Single Call with Function Calling: Best for straightforward tasks where LLM needs to pick the right tool once. Quick to implement.\n*   Agent with Memory & Multi-Step Reasoning: Goes beyond the single step, incorporating iterative decision-making and maintaining a stable state across multiple calls. Can orchestrate complex workflows & calling different tools as new information comes in.\n\nBoth approaches leverage function calling. But while a basic LLM call solves one-off tasks, a full agent adds layers of logic, memory, and iterative planning \u2013 enabling more sophisticated, adaptive solutions.\n\nBuilding an Agent: Code vs. Framework\n-------------------------------------\n\nYou can either build your agent from scratch or use a framework like smolagents, LangGraph, or AutoGen. Frameworks handle common challenges\u2014such as state, routing, and skill descriptions\u2014while offering established best practices and examples.\n\n**Why Use a Framework?**\n\n*   Quick Setup: Skip boilerplate and get started faster.\n*   Docs & Examples: Larger frameworks often have great resources.\n*   Integration: Many tie seamlessly into orchestration libraries like LangChain or LlamaIndex.\n\n**Potential Drawbacks**\n\n*   Reduced Flexibility: Complex agents may outgrow a framework\u2019s abstractions.\n*   Opinionated Designs: Adjusting the framework to fit a unique architecture can be difficult.\n*   Lock-In Concerns: Switching away can mean a major refactor.\n\nWhen to Go Custom\n-----------------\n\nFor highly specialized or large-scale architectures, coding your own agent might be a better fit. It\u2019s easier to fine-tune every layer and avoid limitations imposed by a framework\u2019s design.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc": {"__data__": {"id_": "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df8ec48c-95ba-4faa-9e89-b3e72cde44fe", "node_type": "1", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "a75f8f99b567e952e71ac1fc6196defee371a512071be6a361af64d64c4f7982", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3020f785-8d80-42a7-a99f-a04ebd245dd4", "node_type": "1", "metadata": {}, "hash": "413ba543f84cac2f07e1fe7eeb7bb73915366c897b1e579db4a191ae8230d27c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Frameworks handle common challenges\u2014such as state, routing, and skill descriptions\u2014while offering established best practices and examples.\n\n**Why Use a Framework?**\n\n*   Quick Setup: Skip boilerplate and get started faster.\n*   Docs & Examples: Larger frameworks often have great resources.\n*   Integration: Many tie seamlessly into orchestration libraries like LangChain or LlamaIndex.\n\n**Potential Drawbacks**\n\n*   Reduced Flexibility: Complex agents may outgrow a framework\u2019s abstractions.\n*   Opinionated Designs: Adjusting the framework to fit a unique architecture can be difficult.\n*   Lock-In Concerns: Switching away can mean a major refactor.\n\nWhen to Go Custom\n-----------------\n\nFor highly specialized or large-scale architectures, coding your own agent might be a better fit. It\u2019s easier to fine-tune every layer and avoid limitations imposed by a framework\u2019s design.\n\nBottom Line: If your agent is simple or you want a quick start, frameworks can help. For complex, evolving systems, a custom approach may grant the flexibility you need\u2014though frameworks like smolagents, langgraph, and autogen are steadily improving and may soon fit more intricate use cases.\n\nThe rest of this guide will show you how set up an example agent using the following common frameworks:\n\n*   [smolagents](https://github.com/huggingface/smolagents)\n*   [LangGraph](https://github.com/langchain-ai/langgraph)\n*   [Autogen](https://github.com/microsoft/autogen)\n\nBuild an Agent Using smolagents\n-------------------------------\n\nNext up, let\u2019s explore how quickly we can get an agent running using Hugging Face\u2019s smolagents. We\u2019ll walk through the setup, configuration, and a simple example to highlight just how straightforward it can be.\n\nSmolagents is great because:\n\n*   Pre-build Agents: Has a set of prebuilt common agents to use\n*   Seamless Integration: Tightly connects with Hugging Face tools & infrastructure for streamlined development\n*   Flexible Architecture: Allows for both simple function-calling agents & more intricate workflows\n\n### Step 1: Install the Required Libraries\n\n```\n \npip install smolagent pip install 'smolagents[litellm]'\n```\n\n### Step 2: Import all the Essential Building Blocks\n\nNow let\u2019s bring in the classes and tools we\u2019ll be using:\n\n```\n\nfrom smolagents import (\n   CodeAgent,\n   ToolCallingAgent,\n   ManagedAgent,\n   DuckDuckGoSearchTool,\n   VisitWebpageTool,\n   HfApiModel,\n)\nfrom smolagents import LiteLLMModel \n```\n\nHere\u2019s a quick rundown of what each import is about:\n\n*   **LiteLLMModel and HfApiModel:** Two different model classes; really only need one to build an agent (we\u2019ll focus on using HfApiModel)\n*   **CodeAgent:** Specialized agent to help with code-related tasks (won\u2019t be the focus of this tutorial, but it\u2019s handy to have it available).  \n    ToolCallingAgent: Agent designed to call external tools (ex. web search/code execution)\n*   **ManagedAgent:** Manager for your agent, providing structure around tasks, logging, and more.\n*   **DuckDuckGoSearchTool & VisitWebpageTool:** Predefined tools in the smolagent library that let the agent search the web and visit webpages. You can also create your own custom tools!\n\n### Step 3: Set Up Our Base Models\n\nWe\u2019ll create two model instances\u2014one for a local or custom model, and one for the Hugging Face Hub:\n\n```\n\nmodel = LiteLLMModel(model_id=\"meta-llama/Llama-3.2-3B-Instruct\")\nhfModel = HfApiModel()\n```\n\n*   Model\\_id is just a placeholder; replace it with a different model.\n*   HfApiModel() leverages a Hugging Face model via the Inference API (make sure you have appropriate authentication if needed).\n\n### Step 4: Create Tool Calling Agent\n\n```\n\nagent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=hfModel,\n    add_base_tools=True\n)\n```\n\n**What\u2019s going on here?**\n\n*   tools=\\[DuckDuckGoSearchTool(), VisitWebpageTool()\\]: We give the agent a search tool and a webpage visitation tool so it can explore the web.\n*   model=hfModel: Instructing the agent to use our Hugging Face API model.\n*   add\\_base\\_tools=True: A parameter that adds some basic built-in tools (e.g., for text manipulation).\n\n### Step 5: Run the Agent\n\nNow for the magic moment\u2014let\u2019s see our agent in action.", "mimetype": "text/plain", "start_char_idx": 3957, "end_char_idx": 8179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3020f785-8d80-42a7-a99f-a04ebd245dd4": {"__data__": {"id_": "3020f785-8d80-42a7-a99f-a04ebd245dd4", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc", "node_type": "1", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "43444781a9e9569bcbe99e6ce6c870072544d5bfe66584790202ec783e55070a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "024e5af6-e3db-4146-9402-05442865f3b9", "node_type": "1", "metadata": {}, "hash": "543ba89eee7c2d2ba6abe316075468dd79890834f9c550a8336330c2b12a8da6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   HfApiModel() leverages a Hugging Face model via the Inference API (make sure you have appropriate authentication if needed).\n\n### Step 4: Create Tool Calling Agent\n\n```\n\nagent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=hfModel,\n    add_base_tools=True\n)\n```\n\n**What\u2019s going on here?**\n\n*   tools=\\[DuckDuckGoSearchTool(), VisitWebpageTool()\\]: We give the agent a search tool and a webpage visitation tool so it can explore the web.\n*   model=hfModel: Instructing the agent to use our Hugging Face API model.\n*   add\\_base\\_tools=True: A parameter that adds some basic built-in tools (e.g., for text manipulation).\n\n### Step 5: Run the Agent\n\nNow for the magic moment\u2014let\u2019s see our agent in action. The question we\u2019re asking our agent is:  \n\u201cFetch the share price of Google from 2020 to 2024, and create a line graph from it?\u201d\n\n```\n\nagent.run(\"fetch the share price of google from 2020 to 2024, and create a line graph from it?\")\n```\n\nYour agent will now:\n\n1.  Use DuckDuckGoSearchTool to search for historical share prices of Google.\n2.  Potentially visit pages with the VisitWebpageTool to find that data.\n3.  Attempt to gather information and generate or describe how to create the line graph.\n\nLet\u2019s take it a step further and explore how to manage multiple agents (or keep better tabs on a single agent) by using a ManagedAgent alongside a manager agent. To implement this with our code so far, follow steps 1-3 from above.\n\n### Step 4: Introducing the ManagedAgent (and Manager Agent)\n\nSometimes you want a little more structure around your agent, especially if you plan on juggling multiple tasks or want to keep logs. That\u2019s where the ManagedAgent comes in. For all of these managed agents, you will need a \u201cboss agent\u201d to coordinate tasks \u2013 a manager agent. In the smolagent library, a convenient choice for a manager agent is the CodeAgent.\n\n#### Step 4a: Create your Managed Agents\n\n```\n\n\nsearch_agent = ToolCallingAgent(\n   tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n   model=hfModel,\n)\nmanaged_agent_search = ManagedAgent(\n   agent=search_agent,\n   name=\"search_agent\",\n   description=\"This agent can perform web searches.\"\n)\ncode_worker_agent = CodeAgent(\n   tools=[],\n   model=hfModel,\n)\nmanaged_agent_code = ManagedAgent(\n   agent=code_worker_agent,\n   name=\"coding_agent\",\n   description=\"This agent can handle code-related tasks.\"\n)\n```\n\n#### Step 4b: Create Your Manager Agents\n\n```\n\n\nmanager_agent = CodeAgent(\n   tools=[],\n   model=hfModel,\n   managed_agents=[managed_agent_search, managed_agent_code],\n)\n```\n\n*   Managed Agent: We transform our search\\_agent and code\\_worker\\_agent into ManagedAgents so we can keep better track of what they do (e.g., logging, chaining tasks, etc.).\n*   Manager Agent: This final \u201cboss\u201d agent is a CodeAgent that has both managed\\_agent\\_search and managed\\_agent\\_code under its watch.\n\n### Step 5: Run the Agent\n\n```\n\n\nmanager_agent.run(\"Check the current weather in New York, then explain how to write a basic Python function.\")\n```\n\nWhen you call manager\\_agent.run(\u2026), it can delegate parts of the request to the appropriate underlying agent(s). That\u2019s it! With a ManagedAgent and a manager agent, you can keep your AI agents organized and ready for more complex workflows.\n\nCongrats! We just finished our first agent!\n\nAfter you\u2019ve built your agent, track how it\u2019s doing on Phoenix with our [smolagents integration](https://docs.arize.com/phoenix/tracing/integrations-tracing/hfsmolagents).\n\nBuild an Agent Using AutoGen\n----------------------------\n\nNext up, let\u2019s explore how quickly we can get an agent running using the Autogen Framework. We\u2019ll walk through the setup, configuration, and a simple example to highlight just how straightforward it can be.\n\n### Step 1: Install the Required Libraries\n\n```\n\n\npip install pyautogen -q\npip install -q openai autogen\n```\n\n### Step 2: Import all the Essential Building Blocks\n\nNow let\u2019s bring in the classes and tools we\u2019ll be using:\n\n```", "mimetype": "text/plain", "start_char_idx": 7434, "end_char_idx": 11437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "024e5af6-e3db-4146-9402-05442865f3b9": {"__data__": {"id_": "024e5af6-e3db-4146-9402-05442865f3b9", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3020f785-8d80-42a7-a99f-a04ebd245dd4", "node_type": "1", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "1b9fc447e6d61bc8892177af8727f07f2a42c0011d330ef72ebaf36a080a3555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e58b1ac-4c33-4696-8b5d-e50f7b74846e", "node_type": "1", "metadata": {}, "hash": "28c942a10f09c16f3d682541f3eb3f7e20bc4bc2e3c9c9d8746c23a29b79f36a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pip install pyautogen -q\npip install -q openai autogen\n```\n\n### Step 2: Import all the Essential Building Blocks\n\nNow let\u2019s bring in the classes and tools we\u2019ll be using:\n\n```\n\n\nimport openai\nfrom autogen import AssistantAgent, UserProxyAgent\n```\n\nHere\u2019s a quick rundown of the imports:\n\n*   **AssistantAgent:** Solves tasks with LLMs.\n*   **UserProxyAgent:** Provides feedback to other agents.\n\n### Step 3: Set up your OpenAI API Key & Model\n\nSince AutoGen is backed by an OpenAI Assistant API, you\u2019ll need to set it up prior to creating an agent.\n\n```\n\n\nopenai.api_key =  \n\nconfig_list = [\n   {\n       \"model\": \"gpt-4o\",\n       \"api_key\": openai.api_key,\n   }\n]\n```\n\nConfig\\_list: tells agent which model to use and the associated API key. Most useful if you plan to switch between different model chaining.\n\n### Step 4: Create the AssistantAgent\n\n```\n\n\nwriter_agent = AssistantAgent(\n   name=\"writer\",\n   llm_config={\n       \"config_list\": config_list,\n       \"temperature\": 0.7,\n   }\n)\neditor_agent = AssistantAgent(\n   name=\"editor\",\n   llm_config={\n       \"config_list\": config_list,\n       \"temperature\": 0.0,\n   }\n)\n```\n\nWhat\u2019s going on here?\n\n*   These agents are essentially your AI \u2018bots,\u2019 each configured using the model information in the config\\_list.\n\n### Step 5: Create the UserProxyAgent\n\n```\n\n\nuser_proxy = UserProxyAgent(\n   name=\"user_proxy\",\n   human_input_mode=\"NEVER\"\n)\n```\n\n*   This UserProxyAgent acts on behalf of the human user. This is the one that sends questions or tasks to the assistant.\n*   The human\\_input\\_mode = \u201cNEVER\u201d indicated that there will not be interactive user prompts in real time.\n\n### Step 6: Run the Agent\n\n```\n\n\nstory_prompt = (\"Please write a short story about a brave explorer in space who encounters mysterious alien life and learns a valuable lesson about love.\")\nstory = user_proxy.initiate_chat(\n   writer_agent,\n   message=story_prompt,\n   max_turns=1)\nfeedback_prompt = (\"Please review the following story and provide constructive feedback along with suggestions for improvement: \"+ str(story))\n\nfeedback = user_proxy.initiate_chat(\n   editor_agent,\n   message=feedback_prompt,\n   max_turns=1)\n```\n\nWhen you call user\\_proxy.initiate\\_chat(\u2026), it can delegate parts of the request to the appropriate underlying agent. It\u2019s where the actual \u2018chat\u2019 begins.\n\nThis example demonstrates a two-step process using two specialized agents\u2014a writer and an editor. First, the user\\_proxy agent asks the writer\\_agent to compose a short story about a space explorer who learns a lesson on friendship. Next, it passes that story to the editor\\_agent for constructive feedback and suggestions. This showcases how multiple agents can collaborate to create and refine content in a single workflow.\n\nThat\u2019s it! By following these steps, you\u2019ll have a basic Autogen setup where a UserProxyAgent can send messages to an AssistantAgent configured with the chosen model. You can easily extend this pattern to include multiple agents, tools, or conversation flows as your project grows.\n\nAfter you\u2019ve built your agent, track how it\u2019s doing on Phoenix with our [AutoGen integration](https://docs.arize.com/phoenix/tracing/integrations-tracing/autogen-support).\n\nBuild an Agent Using LangGraph\n------------------------------\n\nNext up, let\u2019s explore how quickly we can get an agent running using the LangGraph Framework. We\u2019ll walk through the setup, configuration, and a simple example to highlight just how straightforward it can be.\n\n### Step 1: Install the Required Libraries\n\n```\n\n\npip install -q langchain-core langchain-community openai langchain_openai\n```\n\n### Step 2: Import all the Essential Building Blocks & Set up OpenAI\n\nNow let\u2019s bring in the classes and tools we\u2019ll be using:\n\n```\n\n\nimport os \nfrom langgraph.graph import START, END, StateGraph \nfrom langchain_core.messages import AIMessage, HumanMessage \nfrom langgraph.graph.message import add_messages \nfrom langchain_community.chat_models import ChatOpenAI \nfrom typing_extensions import TypedDict\n\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n\n```\n\nHere\u2019s a quick rundown of the imports:\n\n*   We bring in classes and functions from langgraph, langchain, and typing to define and manage our workflow.\n\n### Step 3: Define State Type & Create Workflow\n\n```", "mimetype": "text/plain", "start_char_idx": 11262, "end_char_idx": 15511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e58b1ac-4c33-4696-8b5d-e50f7b74846e": {"__data__": {"id_": "8e58b1ac-4c33-4696-8b5d-e50f7b74846e", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "024e5af6-e3db-4146-9402-05442865f3b9", "node_type": "1", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "bdc73b51aaad7a46674aaca90e9d35e2aa74a8bb4e82120df00af94a353fa2e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1be8a2b8-fc43-43a9-bc6a-6eed85923147", "node_type": "1", "metadata": {}, "hash": "ec64de47da4a44e09ff9d68444a93fff8788a03e1e7452ec8e62e535bf936b1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pip install -q langchain-core langchain-community openai langchain_openai\n```\n\n### Step 2: Import all the Essential Building Blocks & Set up OpenAI\n\nNow let\u2019s bring in the classes and tools we\u2019ll be using:\n\n```\n\n\nimport os \nfrom langgraph.graph import START, END, StateGraph \nfrom langchain_core.messages import AIMessage, HumanMessage \nfrom langgraph.graph.message import add_messages \nfrom langchain_community.chat_models import ChatOpenAI \nfrom typing_extensions import TypedDict\n\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n\n```\n\nHere\u2019s a quick rundown of the imports:\n\n*   We bring in classes and functions from langgraph, langchain, and typing to define and manage our workflow.\n\n### Step 3: Define State Type & Create Workflow\n\n```\n\n\nclass State(TypedDict):\n   messages: list\nworkflow = StateGraph(State)\n```\n\n*   We create a State object that will be passed from node to node. It holds a list of messages representing the conversation history.\n*   A StateGraph is the core of our workflow. We specify State as the type of data that the graph manages.\n\n### Step 4: Helper Function for Extracting Content\n\n```\n\n\ndef extract_content(response) -> str:\n   return response.content if hasattr(response, \"content\") else str(response)\n```\n\n**What\u2019s going on here?**\n\n*   When the LLM returns a response, it might have different attributes. This function standardizes it to a string so we can store or process it consistently.\n\n### Step 5: Define Nodes\n\nWe\u2019ll have several nodes in our graph\u2014each representing a step in our travel-planning workflow.\n\n#### 5.1 Destination Recommender\n\n```\n\n\ndef destination_node(state: State) -> dict[str, list[AIMessage]]:\n   llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.6)\n   prompt = state[\"messages\"]\n[-1].content\n   response = llm.invoke(prompt)\n   return {\"messages\": [AIMessage(content=extract_content(response))]}\n\nworkflow.add_node(\"destination\", destination_node)\n```\n\n*   We use a GPT-4\u2013like model (temperature 0.6) to recommend a travel destination based on the user\u2019s prompt.\n*   The output is wrapped in an AIMessage and appended to the conversation state\n\n#### 5.2 Itinerary Builder\n\n```\n\n\ndef itinerary_node(state: State) -> dict[str, list[AIMessage]]:\n   llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.7)\n   destination = state[\"messages\"]\n[-1].content\n   prompt = (\n       \"Based on the destination recommendation below, generate a detailed 5-day itinerary for a traveler. \"\n       \"Include daily attractions, local cuisine suggestions, and leisure activities.\\n\\n\"\n       \"Destination Recommendation:\\n\" + destination\n   )\n   response = llm.invoke(prompt)\n   return {\"messages\": [AIMessage(content=extract_content(response))]}\n\nworkflow.add_node(\"itinerary\", itinerary_node)\n```\n\n*   We use the last message (the recommended destination) to build a prompt for a 5-day itinerary.\n*   The LLM\u2019s response is transformed into an AIMessage representing the itinerary details.\n\n#### 5.3 Expense Estimator\n\n```\n\n\ndef expense_node(state: State) -> dict[str, list[AIMessage]]:\n   llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.0)\n   itinerary = state[\"messages\"]\n[-1].content\n   prompt = (\n       \"Based on the following 5-day itinerary, provide an approximate expense estimate for the entire trip. \"\n       \"Consider costs such as accommodation, meals, local transportation, and activities.\\n\\n\"\n       \"Itinerary:\\n\" + itinerary\n   )\n   response = llm.invoke(prompt)\n   return {\"messages\": [AIMessage(content=extract_content(response))]}\n\nworkflow.add_node(\"expense\", expense_node)\n```\n\n*   We take the itinerary and calculate approximate travel expenses.\n*   A lower temperature (0.0) aims for more consistent, factual output.\n\n### Step 6: Define Workflow Edges & Compile\n\n```\n\n\nworkflow.add_edge(START, \"destination\")\nworkflow.add_edge(\"destination\", \"itinerary\")\nworkflow.add_edge(\"itinerary\", \"expense\")\nworkflow.add_edge(\"expense\", END)\n\ncompiled_app = workflow.compile()\n```\n\n**What\u2019s going on here?**\n\n*   We lay out the sequence in which nodes execute.\n*   The workflow starts at START, then \u201cdestination\u201d \u2192 \u201citinerary\u201d \u2192 \u201cexpense\u201d \u2192 END.\n\n### Step 7: Define Workflow Edges & Compile\n\n```", "mimetype": "text/plain", "start_char_idx": 14783, "end_char_idx": 18929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1be8a2b8-fc43-43a9-bc6a-6eed85923147": {"__data__": {"id_": "1be8a2b8-fc43-43a9-bc6a-6eed85923147", "embedding": null, "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d", "node_type": "4", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9c58b5e838fafac7a4c4e96129486c7b52327bdc2bc5cad4363589ab718a6431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e58b1ac-4c33-4696-8b5d-e50f7b74846e", "node_type": "1", "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "2e023f2d22b4e4f0722d0a38c61f6215d5a60e84b3d50463553e93226bca6148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "workflow.add_edge(START, \"destination\")\nworkflow.add_edge(\"destination\", \"itinerary\")\nworkflow.add_edge(\"itinerary\", \"expense\")\nworkflow.add_edge(\"expense\", END)\n\ncompiled_app = workflow.compile()\n```\n\n**What\u2019s going on here?**\n\n*   We lay out the sequence in which nodes execute.\n*   The workflow starts at START, then \u201cdestination\u201d \u2192 \u201citinerary\u201d \u2192 \u201cexpense\u201d \u2192 END.\n\n### Step 7: Define Workflow Edges & Compile\n\n```\n\n\nworkflow.add_edge(START, \"destination\")\nworkflow.add_edge(\"destination\", \"itinerary\")\nworkflow.add_edge(\"itinerary\", \"expense\")\nworkflow.add_edge(\"expense\", END)\n\ncompiled_app = workflow.compile()\n```\n\n*   We lay out the sequence in which nodes execute.\n*   The workflow starts at START, then \u201cdestination\u201d \u2192 \u201citinerary\u201d \u2192 \u201cexpense\u201d \u2192 END\n\n### Step 8: Invoke Workflow & Retrieve Final Output\n\n```\n\n\ninitial_prompt = (\n   \"I have a budget of $3000 for a 5-day vacation. I enjoy nature, history, and trying local cuisine. I'm open to international travel in early summer. What destination would you recommend?\"\n)\nresult_state = compiled_app.invoke({\"messages\": [HumanMessage(content=initial_prompt)]})\n\nfinal_output = result_state[\"messages\"]\n[-1].content\nprint(final_output)\n```\n\nThat\u2019s it! By following these steps, you\u2019ll have a LangGraph setup where the user\u2019s initial travel criteria flows through a sequence of nodes\u2014destination recommendation, itinerary building, and expense estimation\u2014all orchestrated by a single workflow. This approach is also highly scalable\u2014you can add or remove nodes as your application grows in complexity\u2014and easily personalized by tweaking prompts, changing model parameters, or adding custom decision logic to tailor the user experience.\n\nOnce you\u2019ve built your agent, see how it\u2019s doing in Phoenix with our [LangGraph Integration](https://docs.arize.com/phoenix/tracing/integrations-tracing/langgraph).\n\nConclusion\n----------\n\nBuilding an agent involves balancing simplicity and power\u2014from single-function LLM calls all the way up to complex, multi-node workflows. Whether you\u2019re using a lightweight framework like smolagents, leveraging structured outputs in AutoGen, or orchestrating multiple steps in LangGraph, the key lies in choosing the right abstraction for your problem. Once you\u2019ve set up the necessary routing logic, integrated external tools, and added memory or iterative planning, you\u2019ll have an agent that can tackle real-world tasks with minimal manual intervention. As the space evolves, keep experimenting, stay flexible, and refine your approach to deliver the best possible user experience. Happy building!\n\n[Get started with Phoenix Tracing Integrations](https://docs.arize.com/phoenix/tracing/integrations-tracing)", "mimetype": "text/plain", "start_char_idx": 18513, "end_char_idx": 21203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"a1ba5c18-0c6c-4dd8-9a0f-a126abc0481d": {"node_ids": ["df8ec48c-95ba-4faa-9e89-b3e72cde44fe", "3238e0ad-6aea-47a7-95fc-fa8bc3ed53bc", "3020f785-8d80-42a7-a99f-a04ebd245dd4", "024e5af6-e3db-4146-9402-05442865f3b9", "8e58b1ac-4c33-4696-8b5d-e50f7b74846e", "1be8a2b8-fc43-43a9-bc6a-6eed85923147"], "metadata": {"file_path": "arize_blogs/how_to_build_an_ai_agent.md", "file_name": "how_to_build_an_ai_agent.md", "file_type": "text/markdown", "file_size": 21431, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}}}}