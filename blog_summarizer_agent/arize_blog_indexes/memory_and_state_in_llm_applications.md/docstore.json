{"docstore/metadata": {"573d58d0-764c-463d-a33b-835ed3065377": {"doc_hash": "ad306cf9e65f27568205360f96fe34c0b86a46d910ca84e9c97b6a5e4d40453a"}, "23eb119a-f59f-45c5-8a90-e94805638013": {"doc_hash": "d85b630e5f07b57978f6085de263181e6aa1b81a3f218e3ddd5c7c7f7628833e", "ref_doc_id": "573d58d0-764c-463d-a33b-835ed3065377"}, "60589ffe-58d4-4fb1-b226-52069f194458": {"doc_hash": "c0e75b1e7bad08ea777953382ae52bb226b1c19d98e38f9c793650091655c4f2", "ref_doc_id": "573d58d0-764c-463d-a33b-835ed3065377"}, "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce": {"doc_hash": "20ae7958bd2eecc0fcbfe6420d3a1a65add6154bb440d7b58dd2cd0b08b8e26e", "ref_doc_id": "573d58d0-764c-463d-a33b-835ed3065377"}, "61f0f9d2-22bb-41a5-892f-73e0530ac106": {"doc_hash": "0464ee652af9a4fa221a7c3d61d9080faf6177940d6f3870bbcaa2583b97db58", "ref_doc_id": "573d58d0-764c-463d-a33b-835ed3065377"}}, "docstore/data": {"23eb119a-f59f-45c5-8a90-e94805638013": {"__data__": {"id_": "23eb119a-f59f-45c5-8a90-e94805638013", "embedding": null, "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "573d58d0-764c-463d-a33b-835ed3065377", "node_type": "4", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "ad306cf9e65f27568205360f96fe34c0b86a46d910ca84e9c97b6a5e4d40453a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60589ffe-58d4-4fb1-b226-52069f194458", "node_type": "1", "metadata": {}, "hash": "cdfb474ae5b7e3ce39db6dc10f21d9c6046cb5e6e2826685d993279b4614eb69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Memory and State in LLM Applications\n\nURL Source: http://arize.com/blog/memory-and-state-in-llm-applications/\n\nPublished Time: 2025-02-26T18:18:09+00:00\n\nMarkdown Content:\nMemory in LLM applications is a broad and often misunderstood concept. In this blog, I\u2019ll break down what memory really means, how it relates to state management, and how different approaches\u2014like session-based memory versus long-term persistence\u2014affect performance, cost, and user experience.\n\nDefining \u201cMemory\u201d\n-----------------\n\nThe term \u201cmemory\u201d is often used in discussions about LLM applications, but it\u2019s an abstraction that can mean different things to different people. Broadly, memory refers to any mechanism by which an LLM application stores and retrieves information for future use. This can encompass state in its many forms:\n\n*   **Persisted state:** Data stored in external databases or other durable storage systems\n*   **In-application state:** Information retained only during the active session, which disappears when the application restarts.\n\nLots of people use the word \u201cmemory\u201d without defining it, so take it with a grain of salt when someone uses it (including this article).\n\nWhat is State in LLM Applications?\n----------------------------------\n\nState is the mechanism which a system retains and uses information over time, connecting past interactions to current behavior. In software engineering, state can be categorized into two main types:\n\n*   Stateful systems retain context\n*   Stateless systems treat each interaction independently, with no memory of prior exchanges.\n\nLLM models are stateless. They process each query as a standalone task, generating outputs based solely on the current input. This simplifies model architecture but poses challenges for LLM applications that require context continuity.\n\nFor example, in a chatbot, messages within the same session may appear stateful, as they influence ongoing interactions. However, once the session ends, the application reverts to a stateless design, if there is no information persisted across sessions.\n\nWhy Managing Memory & State is Essential\n----------------------------------------\n\nApplications require state to deliver consistent, coherent, and efficient user experiences. Without it:\n\n*   Users are forced to repeat information\n*   Applications can\u2019t adapt to ongoing context\n*   Processing redundant information increases costs\n\nManaging state requires balancing the things you need, and the things you don\u2019t for the particular LLM application. For applications needing long-term context, such as personalized workflows or multi-session tasks, persisted state is essential. For simpler or transient interactions, lightweight in-memory state might be good enough.\n\nHow to Choose the Best Strategy\n-------------------------------\n\nFirst, a disclaimer: The best way to manage state in an LLM application depends on your specific use case. Different applications require different approaches to state and memory, ranging from transient session-based contexts to long-term persisted data. Choosing the right strategy starts with understanding your actual requirements and balancing trade-offs effectively.\n\n### Parameters for Memory\n\nWhen deciding the larger parameters for memory, it\u2019s a lot like human memory:\n\n*   _How Much to Remember \u2013 Amount of Information:_ Context windows, cost, scalability for state management might be factors to consider\n*   _What to Remember \u2013 Importance:_ Not always binary, could include systems of importance (special entities / memory variables, most recent vs most relevant, or a hierarchy of state, etc. \u2013 all which we\u2019ll get to later)\n\nGenerally the two big ideas above are good abstractions to think about how memory can be broken down. Theoretically, you would either just remember everything, or remember all important things and nothing else, and that could solve any issue, but of course, the real world is going to be some optimal combination of the two.\n\n### LLM State Management Considerations\n\nBelow are more tangible ways to think about state in LLM applications state management mechanics, and how they might affect how that system works\n\n**The Context in Which State Should be Carried**\n\nThe context in which state should be carried will affect the design of your state management system, illustrative examples below.\n\n_End User Examples:_\n\n*   **State Across Messages:** Maintains continuity during an ongoing session but resets once the session ends. Useful for chatbots, iterative problem-solving, or workflows.\n*   **State Across Sessions:** Persists information across sessions, enabling personalization or long-term task tracking. Essential for applications like virtual assistants or collaborative tools.\n\n_Agent Examples:_\n\n*   **State Across Tool Calls:** Does each tool execution need to carry over context, or can the tools operate independently?\n*   **State Across Multiple Agents:** In multi-agent systems, should agents share state, or should they operate with isolated knowledge?\n\n**LLM Model Context Window**\n\nLLMs have a fixed context window, limiting how much information they can process at once.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60589ffe-58d4-4fb1-b226-52069f194458": {"__data__": {"id_": "60589ffe-58d4-4fb1-b226-52069f194458", "embedding": null, "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "573d58d0-764c-463d-a33b-835ed3065377", "node_type": "4", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "ad306cf9e65f27568205360f96fe34c0b86a46d910ca84e9c97b6a5e4d40453a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23eb119a-f59f-45c5-8a90-e94805638013", "node_type": "1", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "d85b630e5f07b57978f6085de263181e6aa1b81a3f218e3ddd5c7c7f7628833e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce", "node_type": "1", "metadata": {}, "hash": "02043f18cb9fbcdd8f5d92da9ae1970e7f852fd44ad0484164d40f72a94b01ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "_End User Examples:_\n\n*   **State Across Messages:** Maintains continuity during an ongoing session but resets once the session ends. Useful for chatbots, iterative problem-solving, or workflows.\n*   **State Across Sessions:** Persists information across sessions, enabling personalization or long-term task tracking. Essential for applications like virtual assistants or collaborative tools.\n\n_Agent Examples:_\n\n*   **State Across Tool Calls:** Does each tool execution need to carry over context, or can the tools operate independently?\n*   **State Across Multiple Agents:** In multi-agent systems, should agents share state, or should they operate with isolated knowledge?\n\n**LLM Model Context Window**\n\nLLMs have a fixed context window, limiting how much information they can process at once. As the input grows, performance can degrade, costs increase, and hallucinations become more frequent. State management strategies must account for these limitations to ensure relevance without exceeding token context windows.\n\n**Costs**\n\nStateful designs often improve user experience and application performance but come with increased costs. Persisted data and larger prompts increase storage and processing demands. Balancing the cost of maintaining state against its benefits is critical for sustainable applications.\n\n**LLM Application Performance**\n\nDepending on the outcome you are trying to produce, some folks might need more complex state management to actually achieve the outcome they desire while others can implement something more simple and still achieve their required results. Some can go with a more brute force approach while others might need to refine the state system.\n\nBasic Ideas: Managing Context\n-----------------------------\n\nExcessive state leads to inefficiencies and irrelevance, while insufficient state disrupts continuity and user satisfaction. A complex state management system can cost a lot of upfront development work and may increase complexity, while a simpler state management might increase model costs and reduce performance.\n\nA thoughtful approach selectively retains what\u2019s most important while discarding what\u2019s unnecessary. There is no right or wrong answer here as this is very much application dependent. There are pros/cons like all engineering decisions.\n\nThe below is by no means an exhaustive list, mainly an idea of common ideas of how context might be managed. There are many implementations of the below, with varying outcomes depending on the implementation details.\n\n### Conversation History\n\n![Image 1: Conversation history example](https://arize.com/wp-content/uploads/2025/02/image9-1024x507.png)\n\nThis is probably the most MVP of all the examples you will see. It\u2019s a pretty great starting point, but as you move towards a more mature LLM application, it\u2019s likely you\u2019ll need to iterate and improve on this core, but basic approach.\n\nConversation history involves including all past messages\u2014user inputs and model outputs\u2014in subsequent prompts. While straightforward and intuitive, this method quickly runs into problems:\n\n*   Degraded performance: Large prompts reduce model quality.\n*   High costs: Token usage grows quadratically with conversation length.\n*   Context limits: Long histories can exceed the model\u2019s capacity.\n\nThis approach works for short interactions but scales poorly in more complex applications.\n\n### Sliding Window\n\n![Image 2: Conversation history example](https://arize.com/wp-content/uploads/2025/02/image6-845x1024.png)\n\nA sliding window retains only the most recent messages, discarding older ones to maintain a fixed context size.\n\n*   Benefits: Keeps context relevant, stays within model limits, and controls token costs.\n*   Drawbacks: Risks losing important earlier details and struggles with long-term dependencies.\n\nSliding windows balance efficiency and relevance but may require supplementation for applications needing deeper context.\n\n### Combining Strategies\n\nBlending approaches, such as combining recent message windows with past summaries, offers a practical balance. This ensures recent context is preserved while retaining essential historical information.  \nThese foundational techniques provide a starting point for managing state effectively, tailored to the specific needs of your application.\n\n### Storage Types\n\nChoosing the right storage type depends on the persistence requirements, cost, and latency of your application. Here\u2019s an example below\n\n**Ephemeral conversation history**\n\n*   Temporarily stored in application memory, used during a single session. For example, a shopping assistant might track interactions in real time without needing to persist data for future sessions.\n\n**Persistent conversation history**\n\n*   Stored in a durable database or vector store, enabling access to prior context across sessions. This is essential for applications like personal assistants that need long-term memory for personalization.\n*   The persistence storage medium\u2014whether a database, blob storage, knowledge graph, or retrieval-augmented generation (RAG) system\u2014should align with your application\u2019s complexity, latency requirements, and cost constraints.\n\nAdvanced State Management\n-------------------------\n\n### Tiering Memory\n\nNot all information is equally valuable. Tiering memory helps prioritize what to retain and for how long.", "mimetype": "text/plain", "start_char_idx": 4344, "end_char_idx": 9690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce": {"__data__": {"id_": "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce", "embedding": null, "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "573d58d0-764c-463d-a33b-835ed3065377", "node_type": "4", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "ad306cf9e65f27568205360f96fe34c0b86a46d910ca84e9c97b6a5e4d40453a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60589ffe-58d4-4fb1-b226-52069f194458", "node_type": "1", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "c0e75b1e7bad08ea777953382ae52bb226b1c19d98e38f9c793650091655c4f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61f0f9d2-22bb-41a5-892f-73e0530ac106", "node_type": "1", "metadata": {}, "hash": "775890c46a1890364c9a05504dad8222527ec1b5eedd232dc27722c8cc036287", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Storage Types\n\nChoosing the right storage type depends on the persistence requirements, cost, and latency of your application. Here\u2019s an example below\n\n**Ephemeral conversation history**\n\n*   Temporarily stored in application memory, used during a single session. For example, a shopping assistant might track interactions in real time without needing to persist data for future sessions.\n\n**Persistent conversation history**\n\n*   Stored in a durable database or vector store, enabling access to prior context across sessions. This is essential for applications like personal assistants that need long-term memory for personalization.\n*   The persistence storage medium\u2014whether a database, blob storage, knowledge graph, or retrieval-augmented generation (RAG) system\u2014should align with your application\u2019s complexity, latency requirements, and cost constraints.\n\nAdvanced State Management\n-------------------------\n\n### Tiering Memory\n\nNot all information is equally valuable. Tiering memory helps prioritize what to retain and for how long. Here are some examples with different kinds of data:\n\n*   **High-priority data:** Critical user inputs, such as preferences or details likely to impact future queries (e.g., a shopper\u2019s size in a clothing app).\n*   **Medium-priority data:** Summarized responses or intermediate results that help maintain context but aren\u2019t essential.\n*   **Low-priority data:** Temporary or large outputs from tooling calls that are unlikely to be reused.\n\nBalancing between how much and how important the state is, can help guide users when to prune and when not to prune this data. This approach ensures that the most relevant data is preserved while managing storage and retrieval costs effectively.\n\n### Specialized Entities/Memory Variables\n\nDomain-specific applications benefit from storing structured entities as discrete memory variables. The idea is that you may know certain fields or entities that will likely be important for your application, so you would preemptively search and store them in the state system.\n\nFor example:\n\n*   A travel assistant could retain destinations, travel dates, budgets, and hobbies as specialized fields, enabling it to respond contextually (e.g., \u201cCan I ski during my trip?\u201d).\n*   An e-commerce chatbot might extract and save product preferences or delivery details for reuse later in the conversation.\n\nBy focusing on relevancy weighting, specialized entities streamline how applications process and retrieve meaningful data consistently used in that domain or workflow.\n\n### Semantic Switches\n\nWhen a conversation shifts topics, the associated memory or state must adapt. For example:\n\n*   In an e-commerce chatbot, if the user switches from discussing a specific product to a different one, injecting irrelevant details about the first product could confuse workflows.\n*   Semantic switches can help applications detect when context has changed, ensuring the system responds appropriately without overloading the state.\n\n### More Dynamic Write and Reads of State\n\nAdvanced state management separates active memory (used during interactions) from external memory (stored for retrieval). This enables scalable, efficient, and adaptive applications.\n\n*   Smart Write: Automatically adds key external data into the LLM\u2019s active context.\n*   Smart Reading: Retrieves specific data on demand, keeping the context lightweight.\n\nMemory hierarchies organize data into:\n\n*   Core memory: Actively used information in the current interaction.\n*   Archival memory: Persistent storage for long-term or less critical data.\n\nEffective systems prioritize relevance, refine stored data over time, and balance retrieval strategies to optimize performance and cost.\n\nMore advanced frameworks like [Letta](https://docs.letta.com/concepts/letta) are actively developing these systems.\n\nEvaluating State Management\n---------------------------\n\nEffectively evaluating state management is essential for understanding and improving how state and memory impact LLM application performance. Here are some approaches to consider:\n\n#### Evals for State and Memory Performance\n\nUse methods such as running LLMs as judges, incorporating human annotations, or leveraging logic-based evaluations to assess tasks that depend on state payloads, structures, or entities.\n\n#### Measure Persisted State Usage\n\n*   Evaluate how often persisted state is accessed to identify unused or unnecessary state.\n*   Consider implementing a time-to-live (TTL) mechanism to remove stale or unused state data.\n*   Measure and address inefficiencies where state is stored but rarely used, refining the system as needed.\n\n#### Component-Level State Evaluation\n\n*   Assess individual components within the larger application to identify where state management can be improved.\n*   Recognize that some components may require nuanced state handling rather than an all-or-nothing approach.\n*   Focus on refining state management for specific parts of the application that rely heavily on state to enhance the overall system.\n\nState management in complex LLM applications is rarely straightforward. By measuring and refining how state is accessed, maintained, and optimized across different components, you can ensure that your system balances performance, cost, and scalability.\n\nState in Different Agent Architectures\n--------------------------------------\n\nHere are some illustrative examples of how state might be handled in various architectures.", "mimetype": "text/plain", "start_char_idx": 8646, "end_char_idx": 14115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61f0f9d2-22bb-41a5-892f-73e0530ac106": {"__data__": {"id_": "61f0f9d2-22bb-41a5-892f-73e0530ac106", "embedding": null, "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "573d58d0-764c-463d-a33b-835ed3065377", "node_type": "4", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "ad306cf9e65f27568205360f96fe34c0b86a46d910ca84e9c97b6a5e4d40453a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce", "node_type": "1", "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "20ae7958bd2eecc0fcbfe6420d3a1a65add6154bb440d7b58dd2cd0b08b8e26e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Consider implementing a time-to-live (TTL) mechanism to remove stale or unused state data.\n*   Measure and address inefficiencies where state is stored but rarely used, refining the system as needed.\n\n#### Component-Level State Evaluation\n\n*   Assess individual components within the larger application to identify where state management can be improved.\n*   Recognize that some components may require nuanced state handling rather than an all-or-nothing approach.\n*   Focus on refining state management for specific parts of the application that rely heavily on state to enhance the overall system.\n\nState management in complex LLM applications is rarely straightforward. By measuring and refining how state is accessed, maintained, and optimized across different components, you can ensure that your system balances performance, cost, and scalability.\n\nState in Different Agent Architectures\n--------------------------------------\n\nHere are some illustrative examples of how state might be handled in various architectures. These are just ideas to consider and are not meant to be definitive or correct solutions.\n\n### Augmented LLM\n\n![Image 3: Augmented LLM ](https://arize.com/wp-content/uploads/2025/02/image3-1024x427.png)\n\nIn a simple standalone system like an Augmented LLM with tool calling or RAG, state can be maintained by appending user inputs and application outputs to a conversation payload. If the context window is exceeded, long RAG contexts or tool call results can be summarized. Combining a sliding window approach with summarization helps retain only the most relevant recent interactions, ensuring continuity without exceeding system capacity.\n\nIf state needs to be shared across a session, you can persist and manage it; otherwise, state may not be necessary for simpler, transient interactions.\n\n### Prompt Chaining\n\n### ![Image 4: Prompt Chaining](https://arize.com/wp-content/uploads/2025/02/image4-1024x427.png)\n\nState flows between steps as outputs from one prompt feed into the next. Summarized or structured data helps retain context across steps while keeping prompts efficient and focused. This workflow is simple which also might make the state management also simple.\n\n### Routing\n\n![Image 5: Routing example](https://arize.com/wp-content/uploads/2025/02/image11-1-1024x427.png)\n\nState could track classification decisions, ensuring inputs are directed to the right task or model. Persistent storage can log routing history for analysis or future optimization.\n\n### Parallelization\n\n![Image 6: Parallelization](https://arize.com/wp-content/uploads/2025/02/image10-1-1024x427.png)\n\nState is split across subtasks, with results temporarily stored and programmatically merged. For voting workflows, outputs from multiple attempts are aggregated to ensure reliability.\n\n### Orchestrator-Workers\n\n![Image 7: Orchestrator workers example](https://arize.com/wp-content/uploads/2025/02/image1-1-1024x427.png)\n\nThe orchestrator dynamically tracks the task\u2019s progress and coordinates workers. It retains state by logging subtasks and results, enabling flexible handling of complex or unpredictable workflows.\n\n### Evaluator-Optimizer\n\n![Image 8: Evaluator-optimizer example](https://arize.com/wp-content/uploads/2025/02/image7-1024x427.png)\n\nState is updated iteratively as the evaluator refines outputs. Feedback and prior iterations are stored to guide the optimizer toward improved results.\n\n### Agents\n\n![Image 9: How agents use state](https://arize.com/wp-content/uploads/2025/02/image2-1-1024x427.png)\n\nSee more here: https://www.anthropic.com/research/building-effective-agents\n\nState includes task progress, tool usage, and environmental feedback, maintained persistently or temporarily based on the task\u2019s scope. Agents use this state to adjust plans and ensure continuity over multiple steps.\n\nLooking Ahead: What is the State of _State_?\n--------------------------------------------\n\nFor Arize customers and the Arize AI Copilot we built, we\u2019ve observed that application architectures are often far more complex than their state management systems, which currently rely on simpler approaches, often centered on gathering and maintaining the entire message history and other less complex state systems.\n\nIt\u2019s still early days, but this suggests two possibilities: either state systems in LLM applications have significant room for refinement and intelligence, or simpler state management may remain a preferred approach for many, provided teams are willing to pay the trade-off of increased context window usage.\n\nAs more complex systems evolve, it will be interesting to see how the balance between simplicity, cost, and efficiency shapes the future of state management in SOTA LLM applications.", "mimetype": "text/plain", "start_char_idx": 13086, "end_char_idx": 17822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"573d58d0-764c-463d-a33b-835ed3065377": {"node_ids": ["23eb119a-f59f-45c5-8a90-e94805638013", "60589ffe-58d4-4fb1-b226-52069f194458", "1dfbef16-2a96-4135-a3de-a9e0f04ba5ce", "61f0f9d2-22bb-41a5-892f-73e0530ac106"], "metadata": {"file_path": "arize_blogs/memory_and_state_in_llm_applications.md", "file_name": "memory_and_state_in_llm_applications.md", "file_type": "text/markdown", "file_size": 17898, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}}}}