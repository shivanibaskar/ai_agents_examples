{"docstore/metadata": {"0696b840-dbef-46ba-af58-5b5b5961945a": {"doc_hash": "9f79eba1a4ad8c4b3eb3aa210251bffbcf9eece7533fdfed5a40e47d3309cb57"}, "508085fc-8183-45ac-bbdd-646c215e1c8f": {"doc_hash": "30d092f6724ca85e4bd3cbec57debc5713ffd4b1ea9a81e52ed0060eb7779b58", "ref_doc_id": "0696b840-dbef-46ba-af58-5b5b5961945a"}, "50ac7b58-4167-4aec-86cd-990689b0fefa": {"doc_hash": "7526c962575cd6965c02b457c35671d1874324a7c636435c220192813977a302", "ref_doc_id": "0696b840-dbef-46ba-af58-5b5b5961945a"}}, "docstore/data": {"508085fc-8183-45ac-bbdd-646c215e1c8f": {"__data__": {"id_": "508085fc-8183-45ac-bbdd-646c215e1c8f", "embedding": null, "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0696b840-dbef-46ba-af58-5b5b5961945a", "node_type": "4", "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9f79eba1a4ad8c4b3eb3aa210251bffbcf9eece7533fdfed5a40e47d3309cb57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50ac7b58-4167-4aec-86cd-990689b0fefa", "node_type": "1", "metadata": {}, "hash": "e1caf80e7968e53c9fd8db98127fb848186cc1d1361b59b7f6054388816d2ec8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How DeepSeek is Pushing the Boundaries of AI Development\n\nURL Source: http://arize.com/blog/how-deepseek-is-pushing-the-boundaries-of-ai-development/\n\nPublished Time: 2025-02-21T22:41:15+00:00\n\nMarkdown Content:\nHow do you train an AI model to think more like a human? That\u2019s the challenge DeepSeek is tackling with its latest models, which push the boundaries of reasoning and reinforcement learning.\n\nIn a recent paper reading, SallyAnn DeLucia, a product manager at Arize, and Nick Luzio, a solutions engineer, broke down the key insights from DeepSeek\u2019s research. We dive into DeepSeek, which has been dominating headlines for its significant breakthrough in inference speed over other models. What\u2019s next for AI (and open source)? From training strategies to real-world performance, here\u2019s what you need to know.\n\nWatch\n-----\n\nListen\n------\n\nResources\n---------\n\n*   DeepSeek [API docs](https://api-docs.deepseek.com/)\n*   [Sign up for future paper readings](https://arize.com/resource/community-papers-reading/)\n\nSummary\n-------\n\n### AI Industry Updates: The Bigger Picture\n\nBefore diving into DeepSeek, we covered some of the latest trends shaping the AI landscape:\n\n*   **OpenAI\u2019s Deep Research** \u2013 A new autonomous search tool powered by OpenAI\u2019s upcoming o3 reasoning model is on the horizon.\n*   **DeepSeek\u2019s Training Costs** \u2013 Initial estimates suggested DeepSeek\u2019s training costs were around $5 million, but new data is challenging that figure.\n*   **AI Spending Surge** \u2013 Amazon, Google, Meta, and Microsoft have spent a staggering $300 billion on AI infrastructure.\n*   **AI Goes Mainstream** \u2013 AI-generated content is appearing in Super Bowl ads, art, and music, sparking debates about authenticity and ethics.\n\n### What is DeepSeek?\n\nDeepSeek\u2019s mission is clear: improve AI reasoning through reinforcement learning. Their latest models\u2014DeepSeek R.1 and R.1.0\u2014explore how RL can be used to refine reasoning without the need for traditional pretraining.\n\nDeepSeek Model Evolution:\n\n*   DeepSeek R.1.0 \u2013 Trained purely through reinforcement learning, with no pretraining. It showcased impressive reasoning abilities but struggled with language consistency.\n*   DeepSeek R.1 \u2013 Introduced supervised fine-tuning before RL, significantly improving language coherence while maintaining strong reasoning performance.\n\nCompetitive Performance \u2013 DeepSeek R.1 performs on par with OpenAI\u2019s O 1 model, even surpassing it in some math-related tasks.\n\nWhy does reasoning matter? It\u2019s a critical step toward Artificial General Intelligence (AGI) and plays a key role in AI agent capabilities.\n\n### How DeepSeek\u2019s Models Work\n\nDeepSeek R.1.0 was trained without human input using reinforcement learning alone. Instead of relying on human annotations, the model was rewarded for:\n\n*   Accuracy \u2013 Correct answers received higher scores.\n*   Formatting \u2013 The model was guided toward structured reasoning, leading to the emergence of \u201cthinking brackets\u201d, a structured way to show its work.\n\nOne fascinating discovery: DeepSeek R.1.0 self-corrected its reasoning process, showing \u201cAha moments\u201d where it adjusted its thought process mid-generation\u2014a behavior not seen in earlier models.\n\nTo address R.1.0\u2019s language inconsistency, DeepSeek R.1 introduced:\n\n*   Supervised fine-tuning \u2013 The model was trained on high-quality, human-annotated data before reinforcement learning.\n*   Improved readability \u2013 Ensured responses were formatted clearly and remained in one language.\n\nWhile DeepSeek R.1 is more readable, it traded off some benchmark performance where R.1.0\u2019s mixed-language approach had an advantage.\n\nTo make DeepSeek more accessible, the team distilled the massive R.1 model into a smaller, more efficient version.\n\n*   Size Reduction \u2013 The original 67-billion-parameter model was distilled into a 1.5-billion-parameter version.\n*   Local Demo \u2013 A Streamlit app demonstrated DeepSeek running locally on an M3 Mac, showcasing its ability to solve math problems with detailed reasoning.\n\n**Latency and Performance:**\n\n*   The 14-billion-parameter model was powerful but slow.\n*   The 1.5-billion-parameter model was faster and more efficient, making it ideal for local deployment where speed and resource efficiency matter.\n\n**Why Run AI Locally?**\n\n*   Privacy \u2013 No need to send data to the cloud.\n*   Customization \u2013 Fine-tune models for specific tasks.\n*   Hardware Constraints \u2013 Ensures models fit within available system resources.\n\n### Use Cases and Considerations\n\nDeepSeek\u2019s models are still in the early stages of real-world adoption, but potential applications include:\n\n*   Enterprise AI \u2013 Some companies are testing DeepSeek models for production use.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50ac7b58-4167-4aec-86cd-990689b0fefa": {"__data__": {"id_": "50ac7b58-4167-4aec-86cd-990689b0fefa", "embedding": null, "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0696b840-dbef-46ba-af58-5b5b5961945a", "node_type": "4", "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "9f79eba1a4ad8c4b3eb3aa210251bffbcf9eece7533fdfed5a40e47d3309cb57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "508085fc-8183-45ac-bbdd-646c215e1c8f", "node_type": "1", "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}, "hash": "30d092f6724ca85e4bd3cbec57debc5713ffd4b1ea9a81e52ed0060eb7779b58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Size Reduction \u2013 The original 67-billion-parameter model was distilled into a 1.5-billion-parameter version.\n*   Local Demo \u2013 A Streamlit app demonstrated DeepSeek running locally on an M3 Mac, showcasing its ability to solve math problems with detailed reasoning.\n\n**Latency and Performance:**\n\n*   The 14-billion-parameter model was powerful but slow.\n*   The 1.5-billion-parameter model was faster and more efficient, making it ideal for local deployment where speed and resource efficiency matter.\n\n**Why Run AI Locally?**\n\n*   Privacy \u2013 No need to send data to the cloud.\n*   Customization \u2013 Fine-tune models for specific tasks.\n*   Hardware Constraints \u2013 Ensures models fit within available system resources.\n\n### Use Cases and Considerations\n\nDeepSeek\u2019s models are still in the early stages of real-world adoption, but potential applications include:\n\n*   Enterprise AI \u2013 Some companies are testing DeepSeek models for production use.\n*   Prompt Engineering \u2013 Switching models may require refining prompts for optimal performance.\n*   Privacy-Focused AI \u2013 DeepSeek\u2019s models offer an alternative for those who need full control over their AI stack.\n*   Traditional ML Tasks \u2013 The 1.5-billion-parameter model works well for sentiment analysis and topic modeling.\n*   AI Agents & Tool Use \u2013 While the smaller models aren\u2019t ideal for tool-calling, larger models might be better suited.\n\n### Final Thoughts: What Makes DeepSeek Unique?\n\nBeyond performance, DeepSeek models have a distinct personality\u2014offering a different experience compared to traditional cloud-based LLMs. Whether you need a reasoning powerhouse or a lightweight local model, DeepSeek\u2019s innovative approach to reinforcement learning is pushing the boundaries of AI development.", "mimetype": "text/plain", "start_char_idx": 3729, "end_char_idx": 5481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"0696b840-dbef-46ba-af58-5b5b5961945a": {"node_ids": ["508085fc-8183-45ac-bbdd-646c215e1c8f", "50ac7b58-4167-4aec-86cd-990689b0fefa"], "metadata": {"file_path": "arize_blogs/exploring_deepseek.md", "file_name": "exploring_deepseek.md", "file_type": "text/markdown", "file_size": 5575, "creation_date": "2025-03-06", "last_modified_date": "2025-03-06"}}}}