{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openinference.instrumentation.langchain in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.1.43)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (0.1.31)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (0.1.17)\n",
      "Requirement already satisfied: opentelemetry-api in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (0.54b1)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference.instrumentation.langchain) (1.17.2)\n",
      "Requirement already satisfied: opentelemetry-sdk in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openinference-instrumentation>=0.1.27->openinference.instrumentation.langchain) (1.33.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api->openinference.instrumentation.langchain) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api->openinference.instrumentation.langchain) (8.6.1)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/codespace/.local/lib/python3.12/site-packages (from opentelemetry-instrumentation->openinference.instrumentation.langchain) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api->openinference.instrumentation.langchain) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/codespace/.local/lib/python3.12/site-packages (from opentelemetry-sdk->openinference-instrumentation>=0.1.27->openinference.instrumentation.langchain) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: unstructured in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: chardet in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (5.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from unstructured) (4.13.3)\n",
      "Requirement already satisfied: emoji in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from unstructured) (2.2.4)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (3.13.0)\n",
      "Requirement already satisfied: backoff in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (0.35.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from unstructured) (7.0.0)\n",
      "Requirement already satisfied: python-oxmsg in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk->unstructured) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: olefile in /usr/local/python/3.12.1/lib/python3.12/site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->unstructured) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->unstructured) (2025.1.31)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured-client->unstructured) (45.0.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured-client->unstructured) (2.11.5)\n",
      "Requirement already satisfied: pypdf>=4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured-client->unstructured) (5.5.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/codespace/.local/lib/python3.12/site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/codespace/.local/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openinference.instrumentation.langchain\n",
    "!pip install unstructured\n",
    "\n",
    "! pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai langchain-text-splitters faiss-cpu 'arize-phoenix[evals]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## Notes\n",
    "- **Arize Phoenix** is an open-source observability tool for LLM apps.\n",
    "- It helps log, evaluate, and trace model inputs and outputs.\n",
    "- Supports metrics like correctness, hallucination, and toxicity.\n",
    "- Great for debugging agents and tracking multi-step reasoning.\n",
    "- Works with LangChain, OpenAI, Hugging Face, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗️ The launch_app `port` parameter is deprecated and will be removed in a future release. Use the `PHOENIX_PORT` environment variable instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
      "  next(self.gen)\n",
      "/usr/local/python/3.12.1/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
      "  next(self.gen)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747971208.560827   48841 chttp2_server.cc:1682] UNKNOWN:No address added out of total 1 resolved for '[::]:4317' {created_time:\"2025-05-23T03:33:28.560821059+00:00\", children:[UNKNOWN:Failed to add any wildcard listeners {created_time:\"2025-05-23T03:33:28.560799129+00:00\", children:[UNKNOWN:Unable to configure socket {created_time:\"2025-05-23T03:33:28.557806286+00:00\", fd:86, children:[UNKNOWN:bind: Address already in use (98) {created_time:\"2025-05-23T03:33:28.550430478+00:00\"}]}, UNKNOWN:Unable to configure socket {created_time:\"2025-05-23T03:33:28.560794931+00:00\", fd:86, children:[UNKNOWN:bind: Address already in use (98) {created_time:\"2025-05-23T03:33:28.560784631+00:00\"}]}]}]}\n",
      "ERROR:    Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py\", line 692, in lifespan\n",
      "    async with self.lifespan_context(app) as maybe_state:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/phoenix/server/app.py\", line 532, in lifespan\n",
      "    await stack.enter_async_context(grpc_server)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n",
      "    result = await _enter(cm)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/phoenix/server/grpc_server.py\", line 108, in __aenter__\n",
      "    server.add_insecure_port(f\"[::]:{get_env_grpc_port()}\")\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/grpc/aio/_server.py\", line 102, in add_insecure_port\n",
      "    return _common.validate_port_binding_result(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/grpc/_common.py\", line 181, in validate_port_binding_result\n",
      "    raise RuntimeError(_ERROR_MESSAGE_PORT_BINDING_FAILED % address)\n",
      "RuntimeError: Failed to bind to address [::]:4317; set GRPC_VERBOSITY=debug environment variable to see detailed error message.\n",
      "\n",
      "ERROR:    Application startup failed. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "🔭 OpenTelemetry Tracing Details 🔭\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This starts the Phoenix observability app \n",
    "#locally (usually at http://localhost:6006) and opens a session you can use to log and inspect data.\n",
    "\n",
    "# 🔥 Launch the Phoenix observability dashboard (usually at http://localhost:6006)\n",
    "px.launch_app(port=6006)\n",
    "\n",
    "# 🧠 Register a tracer provider to collect trace data for LLM activity\n",
    "tracer_provider = register()\n",
    "\n",
    "# 🔗 Connect LangChain to Phoenix using the tracer provider\n",
    "# This lets Phoenix observe and log each step taken by a LangChain agent\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a text splitter that breaks documents into chunks of 1024 characters\n",
    "# with an overlap of 128 characters between each chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "# 2. Load all documents from the '../city_data' directory\n",
    "loader = DirectoryLoader(\"../city_data\")\n",
    "\n",
    "# 3. Load and split the documents using the defined text splitter\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': \"Incorrect API key provided: OPENAI_A*****************************************************************************************************************************************************************************gMA'. You can find your API key at https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m embeddings = OpenAIEmbeddings()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Step 2: Create a FAISS vector store from the list of documents\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# FAISS (Facebook AI Similarity Search) allows fast similarity search over dense vectors\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 'from_documents' will:\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# - Embed each chunk in 'docs'\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# - Store them in a FAISS index for fast retrieval\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m db = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Step 3 (Optional): Perform a similarity search\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Provide a natural language query to retrieve the most relevant document chunks\u001b[39;00m\n\u001b[32m     17\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mTell me about Houston demographics\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    482\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': \"Incorrect API key provided: OPENAI_A*****************************************************************************************************************************************************************************gMA'. You can find your API key at https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Step 1: Initialize the OpenAI embedding model\n",
    "# This model converts each document chunk into a numerical vector\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 2: Create a FAISS vector store from the list of documents\n",
    "# FAISS (Facebook AI Similarity Search) allows fast similarity search over dense vectors\n",
    "# 'from_documents' will:\n",
    "# - Embed each chunk in 'docs'\n",
    "# - Store them in a FAISS index for fast retrieval\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 3 (Optional): Perform a similarity search\n",
    "# Provide a natural language query to retrieve the most relevant document chunks\n",
    "query = \"Tell me about Houston demographics\"\n",
    "results = db.similarity_search(query)\n",
    "print(results[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the FAISS vector store into a retriever object\n",
    "# This allows it to be used in LangChain pipelines like RetrievalQA or ConversationalRetrievalChain\n",
    "retriever = db.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Create a retriever tool from the FAISS retriever\n",
    "# This tool can be used by an agent to search through vector-retrieved city data\n",
    "tool = create_retriever_tool(\n",
    "    retriever,                       # The retriever object (e.g., FAISS as retriever)\n",
    "    \"search_cities\",                # Tool name (how the agent will refer to it)\n",
    "    \"Searches and returns excerpts from Wikipedia entries of many cities.\"  # Tool description\n",
    ")\n",
    "\n",
    "# Add this tool to a list of tools the agent can use\n",
    "tools = [tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Pull a prebuilt prompt from the LangChain Hub.\n",
    "# This prompt is specifically designed for use with the \"openai-tools\" agent type.\n",
    "# The agent needs a structured system prompt to understand:\n",
    "# - What tools it has access to (like a retriever, calculator, etc.)\n",
    "# - How to decide when to use a tool vs. just respond\n",
    "# - How to format tool calls and interpret tool outputs\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "# Display the list of messages that make up the prompt.\n",
    "# The prompt typically includes a system message with tool instructions and\n",
    "# may also include example interactions (e.g., few-shot examples).\n",
    "# These messages will be fed to the Chat model to guide its behavior.\n",
    "prompt.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize a ChatOpenAI model instance (like GPT-3.5 or GPT-4) for use in a LangChain workflow\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,   # Controls randomness. 0 = most deterministic and factual (great for QA tasks)\n",
    "    verbose=True     # If True, LangChain will print out detailed logs of what the LLM is doing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "# Create a tool-using agent using the OpenAI function-calling (tools) capability.\n",
    "# - 'llm' is your ChatOpenAI instance\n",
    "# - 'tools' is a list of tools the agent can use (e.g., your retriever tool)\n",
    "# - 'prompt' is the system message template that tells the agent how to interact with tools\n",
    "agent = create_openai_tools_agent(\n",
    "    llm=llm,        # The language model (e.g., GPT-4)\n",
    "    tools=tools,    # The tools available to the agent (retriever, calculator, etc.)\n",
    "    prompt=prompt   # Prompt pulled from LangChain Hub to guide tool usage\n",
    ")\n",
    "\n",
    "# Wrap the agent in an AgentExecutor\n",
    "# The executor is what you’ll actually call to run the agent and handle input/output\n",
    "# It manages calling the agent, interpreting tool outputs, and looping as needed\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,    # The agent logic (how to choose and use tools)\n",
    "    tools=tools     # Tool definitions (must match what the agent expects)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent_executor with a user input\n",
    "# The agent will:\n",
    "# 1. Read the input question\n",
    "# 2. Decide whether it needs to use any tools (like your retriever)\n",
    "# 3. Use those tools to fetch relevant data (if needed)\n",
    "# 4. Generate a final answer using the LLM\n",
    "\n",
    "result = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Which city has the best food scene out of Lisbon, Houston, Berlin, Moscow, and Paris?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information retrieved, each city has its own unique food scene:\\n\\n- **Lisbon**: Lisbon is known for its diverse food and restaurant culture. It is recognized for its seafood dishes, pastries like Pastel de Nata, and traditional Portuguese cuisine.\\n\\n- **Houston**: Houston is highly regarded for its diverse food scene and is often referred to as the \"Culinary Capital of the South.\" The city offers a wide range of international cuisines due to its diverse population.\\n\\n- **Berlin**: Berlin has a vibrant food scene with a mix of traditional German cuisine and international influences. The city is known for its street food, multicultural eateries, and trendy food markets.\\n\\n- **Moscow**: Moscow offers a mix of traditional Russian cuisine and international dining options. The city is home to a variety of restaurants, cafes, and food markets serving both local and global dishes.\\n\\n- **Paris**: Paris is famous for its culinary heritage and is known as a gastronomic capital. The city offers a wide range of Michelin-starred restaurants, cafes, bakeries, and traditional French cuisine.\\n\\nEach city has its own unique culinary offerings, making it difficult to determine which city has the best food scene as it ultimately depends on personal preferences and tastes.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
